{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Due to the large nature of the data, only a small test sample is uploaded for reproducibility. I have included some of the original outputs/images as markdown/.pngs to better communicate the original workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Used while building util file\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Adding path to util \n",
    "import sys\n",
    "sys.path[-1] = f'{sys.path[0]}'.replace('Notebooks', 'src')\n",
    "\n",
    "#/src/cleaning/cleaning_util.py\n",
    "import cleaning.cleaning_util as clean\n",
    "#/src/visualizations/viz.py\n",
    "from visualizations.viz import hist, class_word_count_plots, word_sims_plots\n",
    "\n",
    "#Pandas preferences\n",
    "clean.pd.set_option('display.max_rows', 500)\n",
    "clean.pd.set_option('display.max_columns', 500)\n",
    "clean.pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = clean.load_process_text('../Data/raw/raw_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "      <th>text</th>\n",
       "      <th>edited</th>\n",
       "      <th>ups</th>\n",
       "      <th>down</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>gilded</th>\n",
       "      <th>awards</th>\n",
       "      <th>sub</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>post_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-14 15:52:12</td>\n",
       "      <td>How can I handle the anxiety that job intervie...</td>\n",
       "      <td>Work/School</td>\n",
       "      <td>I understand that job interviews are stressful...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>How can I handle the anxiety that job intervie...</td>\n",
       "      <td>(how, can, i, handle, the, anxiety, that, job,...</td>\n",
       "      <td>[handle, anxiety, job, interview, cause, under...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-26 07:14:32</td>\n",
       "      <td>How I feel like a failure.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey everyone, longtime watcher first-time sub ...</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>adhd</td>\n",
       "      <td>How I feel like a failure.. Hey everyone, long...</td>\n",
       "      <td>(how, i, feel, like, a, failure, .., hey, ever...</td>\n",
       "      <td>[feel, like, failure, hey, longtime, watcher, ...</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-14 13:03:59</td>\n",
       "      <td>I've always needed upcoming excitement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All my life I've needed something exciting in ...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>adhd</td>\n",
       "      <td>I've always needed upcoming excitement. All my...</td>\n",
       "      <td>(i, 've, always, needed, upcoming, excitement,...</td>\n",
       "      <td>[need, upcoming, excitement, life, need, excit...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-29 23:52:15</td>\n",
       "      <td>i’m not sure what to gift myself for my birthd...</td>\n",
       "      <td>:thinking: Thoughts &amp; Ideas</td>\n",
       "      <td>i’ve never much been the type to ask for anyth...</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>non_clinical</td>\n",
       "      <td>i’m not sure what to gift myself for my birthd...</td>\n",
       "      <td>(i, ’m, not, sure, what, to, gift, myself, for...</td>\n",
       "      <td>[sure, gift, birthday, type, ask, birthday, im...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-22 21:08:44</td>\n",
       "      <td>Free Online Therapy Service?</td>\n",
       "      <td>Advice Needed</td>\n",
       "      <td>This might be a long shot but I was wondering ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Free Online Therapy Service?. This might be a ...</td>\n",
       "      <td>(free, online, therapy, service, ?, ., this, m...</td>\n",
       "      <td>[free, online, therapy, service, long, shot, w...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created                                              title  \\\n",
       "0  2020-07-14 15:52:12  How can I handle the anxiety that job intervie...   \n",
       "1  2018-11-26 07:14:32                         How I feel like a failure.   \n",
       "2  2019-03-14 13:03:59             I've always needed upcoming excitement   \n",
       "3  2020-05-29 23:52:15  i’m not sure what to gift myself for my birthd...   \n",
       "4  2019-01-22 21:08:44                       Free Online Therapy Service?   \n",
       "\n",
       "                         flair  \\\n",
       "0                  Work/School   \n",
       "1                          NaN   \n",
       "2                          NaN   \n",
       "3  :thinking: Thoughts & Ideas   \n",
       "4                Advice Needed   \n",
       "\n",
       "                                                text edited  ups  down  \\\n",
       "0  I understand that job interviews are stressful...  False    4     0   \n",
       "1  Hey everyone, longtime watcher first-time sub ...  False    3     0   \n",
       "2  All my life I've needed something exciting in ...  False    4     0   \n",
       "3  i’ve never much been the type to ask for anyth...  False    3     0   \n",
       "4  This might be a long shot but I was wondering ...  False    1     0   \n",
       "\n",
       "   num_comments  gilded  awards           sub  \\\n",
       "0            10       0       0       anxiety   \n",
       "1             2       0       0          adhd   \n",
       "2             2       0       0          adhd   \n",
       "3             6       0       0  non_clinical   \n",
       "4             0       0       0       anxiety   \n",
       "\n",
       "                                          total_text  \\\n",
       "0  How can I handle the anxiety that job intervie...   \n",
       "1  How I feel like a failure.. Hey everyone, long...   \n",
       "2  I've always needed upcoming excitement. All my...   \n",
       "3  i’m not sure what to gift myself for my birthd...   \n",
       "4  Free Online Therapy Service?. This might be a ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  (how, can, i, handle, the, anxiety, that, job,...   \n",
       "1  (how, i, feel, like, a, failure, .., hey, ever...   \n",
       "2  (i, 've, always, needed, upcoming, excitement,...   \n",
       "3  (i, ’m, not, sure, what, to, gift, myself, for...   \n",
       "4  (free, online, therapy, service, ?, ., this, m...   \n",
       "\n",
       "                                          lemmatized  post_length  \n",
       "0  [handle, anxiety, job, interview, cause, under...          182  \n",
       "1  [feel, like, failure, hey, longtime, watcher, ...          183  \n",
       "2  [need, upcoming, excitement, life, need, excit...          101  \n",
       "3  [sure, gift, birthday, type, ask, birthday, im...          138  \n",
       "4  [free, online, therapy, service, long, shot, w...          157  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "While I didn't plan to do any feature engineering on structural aspects of posts (e.g., post lengths), I felt it negligent to not at least examine a couple basic distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Post distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hist(df['post_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![total length](../reports/figures/output_14_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "hist(df[df['post_length'] < 500]['post_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![total length](../reports/figures/output_15_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Looks like most posts are < 100 words. Could have some difficulty in classification due to post brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class_word_counts = clean.class_count_dict(df, ['depression', 'anxiety', 'adhd', 'non_clinical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class_word_count_plots(2, 2, class_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![total length](../reports/figures/output_20_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Text feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I explored a variety of methods in engineering features, nearly all of which were inspired from this paper: https://white.ucc.asn.au/publications/White2015SentVecMeaning.pdf.\n",
    "\n",
    "They are as follows:\n",
    "1. Distributed memory paragraph vectors (PV-DM)\n",
    "2. Distributed bag of words paragraph vectors (PV-DBOW)\n",
    "3. Mean of word embeddings (MOWE)\n",
    "4. Term frequency - inverse document frequency (Tf-idf)\n",
    "5. Idf-weighted MOWE\n",
    "\n",
    "The PV-DM and PV-CBOW methods were implemented using Gensim's ```Doc2Vec``` . Rather than using a pre-trained model, I chose to train custom embeddings. With approximately 135,000 posts, the dataset was relatively robust, and I felt the niche nature of the subreddits could provide a valuable contribution to mental health based language models. Fortunately, word embeddings were also developed seamlessly during this process. The MOWE method averaged the individual word embeddings for each post, with words not in the ```Doc2Vec``` model's vocabulary fit with 0 vectors. The tf-idf matrix was constructed using this same vocabulary, and the idf weights were also saved separately to weight the individual word embeddings in the final method.\n",
    "\n",
    "Below are 2 classes for these purposes. The first, ```D2V```, does the document tagging and model training with default parameters based on the ```PV-DM``` and ```PV-DBOW``` methods, respectively. Next, ```GetVectors``` contains several methods for computing the MOWEs, Idf-weighted MOWEs, and tf-idf scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "First, I'll split the data into train/test sets. We don't want any test set leakage to occur while training the embeddings/getting the tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = clean.train_test_split(df.drop('sub', axis = 1),\n",
    "                                                                  df['sub'], \n",
    "                                                                  test_size = .2,   \n",
    "                                                                  random_state = 13,  \n",
    "                                                                  stratify = df['sub'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Models & feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dm = clean.D2V(X_train).tag_docs().model_train()\n",
    "dbow = clean.D2V(X_train, model_type = 'dbow').tag_docs().model_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that the models are trained, we have access to all of the ```Doc2Vec``` attributes/methods. As a sanity check, I'm going to run the classic ```King - man + woman = _``` example. The theoretically \"correct\" answer that we should see for the most similar word is \"queen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# print('PV-DM Model:\\n')\n",
    "# print(dm.wv.most_similar_cosmul(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))\n",
    "# print('\\n')\n",
    "# print('PV-DBOW Model:\\n')\n",
    "# print(dbow.wv.most_similar_cosmul(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "PV-DM Model:\n",
    "\n",
    "[('profit', 0.661750853061676), ('prince', 0.6553366780281067), ('washington', 0.6530252695083618), ('atlanta', 0.6519854664802551), ('princess', 0.651204526424408), ('lookin', 0.6439474821090698), ('sopranos', 0.6396740078926086), ('san', 0.6389790177345276), ('brewery', 0.637477457523346), ('percentile', 0.6371206641197205)]\n",
    "\n",
    "\n",
    "PV-DBOW Model:\n",
    "\n",
    "[('queen', 0.6674830317497253), ('stephen', 0.6629737615585327), ('broadway', 0.6586296558380127), ('malcolm', 0.6560490131378174), ('lion', 0.6448732018470764), ('harley', 0.6433538198471069), ('j.', 0.6418041586875916), ('gecko', 0.6416754722595215), ('unfunny', 0.6396648287773132), ('prince', 0.637599766254425)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "The DBOW model is right on the money. Next, I'm going to extract the document vectors from both DBOW and DM models. Moving forward to get MOWE and idf-MOWE embeddings for both models might be overkill, so I'll do a quick logistic regression to compare the DBOW and DM models. I'll use these results to decide which one to extract the final sets of embeddings from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "le = clean.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Creating `GetVectors` object for both models\n",
    "dm_fit = clean.GetVectors(X_train, X_test, model = dm)\n",
    "dbow_fit = clean.GetVectors(X_train, X_test, model = dbow)\n",
    "\n",
    "#PV-DM document vectors\n",
    "dm_doc_vecs_train, dm_doc_vecs_test = dm_fit.d2v_vecs()\n",
    "\n",
    "#PV-DBOW document vectors\n",
    "dbow_doc_vecs_train, dbow_doc_vecs_test = dbow_fit.d2v_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "doc_vec_pairs = [('PV-DM', dm_doc_vecs_train, y_train), ('PV-DBOW', dbow_doc_vecs_train, y_train)]\n",
    "for pair in doc_vec_pairs:\n",
    "    lr = clean.LogisticRegression(multi_class = 'multinomial', max_iter = 1000)\n",
    "    lr.fit(pair[1], pair[2])\n",
    "    print('{} F1 Score: {:.3f}'.format(pair[0], clean.f1_score(lr.predict(pair[1]), pair[2], average = 'macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "PV-DM F1 Score: 0.799\n",
    "PV-DBOW F1 Score: 0.836\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br>\n",
    "Based on these results, I'm going to use the DBOW model for the remainder of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#MOWE vectors\n",
    "dbow_mowe_train, dbow_mowe_test = dbow_fit.w2v_vecs()\n",
    "#Tf-idf\n",
    "tf_fitted = dbow_fit.tfidf_fit()\n",
    "#Tf-idf matrices\n",
    "tfidf_train, tfidf_test = tf_fitted.tfidf_transform()\n",
    "#Idf-MOWE vectors\n",
    "dbow_idf_weighted_train, dbow_idf_weighted_test = tf_fitted.tf_mowe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As a quick check, we could view idf weights of 2 words -- one being a word we think is quite common, and the other being rarer. For example, a word like \"depressed\" should have a lower weight than a word like \"zoloft\" since its frequency across the corpus is presumably much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depressed: 3.902\n",
      "Prescription: 5.740\n"
     ]
    }
   ],
   "source": [
    "print('Depressed: {:.3f}'.format(tf_fitted.words_weights['depressed']))\n",
    "print('Prescription: {:.3f}'.format(tf_fitted.words_weights['prescription']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br>\n",
    "\n",
    "```zoloft``` was originally used but is not available in this corpus sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "Depressed: 3.778\n",
    "Zoloft: 6.209\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, we'll save these before moving onto the modeling. Additionally, I'm saving the actual PV-DBOW model so that after testing, I can add the test set to continue training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# cleaning_util.dump(dbow_doc_vecs_train, '../Data/processed/dbow_vecs_train.joblib')\n",
    "# cleaning_util.dump(dbow_doc_vecs_test, '../Data/processed/dbow_vecs_test.joblib')\n",
    "# cleaning_util.dump(dbow_mowe_train, '../Data/processed/dbow_mowe_train.joblib')\n",
    "# cleaning_util.dump(dbow_mowe_test, '../Data/processed/dbow_mowe_test.joblib')\n",
    "# cleaning_util.dump(tfidf_train, '../Data/processed/tfidf_train.joblib') \n",
    "# cleaning_util.dump(tfidf_test, '../Data/processed/tfidf_test.joblib')\n",
    "# cleaning_util.dump(dbow_idf_weighted_train, '../Data/processed/mowe_idf_train.joblib')\n",
    "# cleaning_util.dump(dbow_idf_weighted_test, '../Data/processed/mowe_idf_test.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Something really cool that we can do with Gensim is get word similarity scores. A good test of how well the embeddings are capturing meaning might be to examine medication word similarities. That is, if similar medications are similar in their vectors.\n",
    "\n",
    "*Unfortunately, these words don't show up in the test sample, and the Doc2Vec model was too big to upload in a zipped folder. If you want to play around with the embeddings, I'm happy to send em on request!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Medications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Adderall = ADHD medication<br>\n",
    "Zoloft = anti-Depressant<br>\n",
    "Xanax = anxiety medication<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# model_dbow.wv.most_similar('adderall', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "[('vyvanse', 0.7835362553596497),\n",
    " ('xr', 0.7615510821342468),\n",
    " ('ritalin', 0.757441520690918),\n",
    " ('concerta', 0.7297625541687012),\n",
    " ('med', 0.7136313915252686)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# model_dbow.wv.most_similar('zoloft', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "[('lexapro', 0.602796196937561),\n",
    " ('mg', 0.589096188545227),\n",
    " ('medication', 0.5872026681900024),\n",
    " ('wellbutrin', 0.5750789046287537),\n",
    " ('sertraline', 0.5705499649047852)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# model_dbow.wv.most_similar('xanax', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "```\n",
    "[('klonopin', 0.5459912419319153),\n",
    " ('benzo', 0.49367237091064453),\n",
    " ('prescribe', 0.48809340596199036),\n",
    " ('ativan', 0.4806302785873413),\n",
    " ('pill', 0.47574320435523987)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "Visualizing two of these: adderall and xanax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# word_sims_plots(1, 2, ['adderall', 'xanax'], dbow, 5, ['#B13000', '#FF7E4E'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![medication similarities](../reports/figures/output_53_0.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
